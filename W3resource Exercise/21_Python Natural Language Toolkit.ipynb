{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a Python NLTK program to split the text sentence/paragraph into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Joe waited for the train. The train was late. \n",
      "Mary and Samantha took the bus. \n",
      "I looked for Mary and Samantha at the bus station.\n",
      "\n",
      "['\\nJoe waited for the train.', 'The train was late.', 'Mary and Samantha took the bus.', 'I looked for Mary and Samantha at the bus station.']\n",
      "['Joe', 'waited', 'for', 'the', 'train', '.', 'The', 'train', 'was', 'late', '.', 'Mary', 'and', 'Samantha', 'took', 'the', 'bus', '.', 'I', 'looked', 'for', 'Mary', 'and', 'Samantha', 'at', 'the', 'bus', 'station', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\MeetKothari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "text = '''\n",
    "Joe waited for the train. The train was late. \n",
    "Mary and Samantha took the bus. \n",
    "I looked for Mary and Samantha at the bus station.\n",
    "'''\n",
    "print(text)\n",
    "\n",
    "sent = sent_tokenize(text)\n",
    "print(sent)\n",
    "\n",
    "words = word_tokenize(text)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write a Python NLTK program to tokenize words, sentence wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Joe', 'waited', 'for', 'the', 'train', '.'],\n",
       " ['The', 'train', 'was', 'late', '.'],\n",
       " ['Mary', 'and', 'Samantha', 'took', 'the', 'bus', '.'],\n",
       " ['I',\n",
       "  'looked',\n",
       "  'for',\n",
       "  'Mary',\n",
       "  'and',\n",
       "  'Samantha',\n",
       "  'at',\n",
       "  'the',\n",
       "  'bus',\n",
       "  'station',\n",
       "  '.']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    "Joe waited for the train. The train was late. \n",
    "Mary and Samantha took the bus. \n",
    "I looked for Mary and Samantha at the bus station.\n",
    "'''\n",
    "\n",
    "words = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
